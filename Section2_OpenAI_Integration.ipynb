{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplar Answer Generation Project - OpenAI Integration Section\n",
    "\n",
    "This notebook preprocesses the training data, formats the data, integrates OpenAI API, and lastly evaluate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependency Import\n",
    "\n",
    "Aimed to assist the later dependencies import in the project. Several libraries and modules will be imported throughout the code for various tasks such as data manipulation, preprocessing, OpenAI API integration, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: openai in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (1.52.1)\n",
      "Requirement already satisfied: tiktoken in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: nltk in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: spacy in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (3.8.2)\n",
      "Requirement already satisfied: textstat in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (0.7.4)\n",
      "Requirement already satisfied: rouge-score in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: click in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: jinja2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from spacy) (3.4.1)\n",
      "Requirement already satisfied: pyphen in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from textstat) (0.16.0)\n",
      "Requirement already satisfied: absl-py in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: language-data>=1.2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.3)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in e:\\software\\anaconda\\envs\\curaproject\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy scikit-learn openai tiktoken nltk spacy textstat rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Training Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Imports for section 1\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DataSet Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 117 training samples\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "with open('data/cura-llm-training-data.json', 'r') as file:\n",
    "    training_data = json.load(file)\n",
    "    \n",
    "    print(f\"Loaded {len(training_data)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Data Preprocessing and Preparation\n",
    "\n",
    "We'll first preprocess our data to clean and standardize it, then format it for the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess a single data item\n",
    "def preprocess_data(data_item):\n",
    "    \n",
    "    processed = {}\n",
    "    \n",
    "    # 1. Clean and truncate task content\n",
    "    task_content = data_item['task_content']\n",
    "    \n",
    "    # Remove HTML entities\n",
    "    task_content = task_content.replace('&nbsp;', ' ')\n",
    "    \n",
    "    # Truncate if too long (keeping most relevant parts)\n",
    "    if len(task_content) > 11000:\n",
    "        task_content = task_content[:11000]\n",
    "    \n",
    "    processed['task_content'] = task_content\n",
    "    \n",
    "    # 2. Format question\n",
    "    processed['question'] = data_item['question'].strip()\n",
    "    \n",
    "    # 3. Process rubric\n",
    "    rubric = json.loads(data_item['rubric'])\n",
    "    \n",
    "    # Normalize scoring criteria\n",
    "    processed['rubric'] = {\n",
    "        'criteria': rubric['criteria'],\n",
    "        'total_score': int(rubric['total_score']),\n",
    "        'items': [item.strip() for item in rubric['items']]\n",
    "    }\n",
    "    \n",
    "    # 4. Clean exemplar answer\n",
    "    answer = data_item['answer']\n",
    "    \n",
    "    # Remove extra quotes\n",
    "    answer = answer.strip('\"')\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    answer = ' '.join(answer.split())\n",
    "    processed['answer'] = answer\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preprocessed 117 items\n"
     ]
    }
   ],
   "source": [
    "# Process all data items\n",
    "processed_data = []\n",
    "\n",
    "for item in training_data:\n",
    "    try:\n",
    "        processed = preprocess_data(item)\n",
    "        processed_data.append(processed)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {e}\")\n",
    "    continue\n",
    "\n",
    "print(f\"Successfully preprocessed {len(processed_data)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of preprocessed item:\n",
      "{\n",
      "  \"task_content\": \"Designing your rocket    Building phase     The shape, weight, and size of a rocket, and it\\u2019s design of nose cone and fins, all affect how aerodynamic or efficient it will be.     Being efficient allows a rocket to use less fuel while travelling long distances or overcoming gravity to take off and escape our atmosphere.     Rockets need to go straight up when launching and not veer to one side or roll when travelling through space. An effective  nose cone  and  fins  will help to stabilise your rocket.     So, to create a rocket that can be launched into space, you must design:     A rocket body    Fins &amp; a nose cone    A final rocket design with all elements     In your teams, you must:     Design your nose cones and fins/tail. Think about which materials to use and how to attach them before constructing their designs    Two team members could construct the nose cone and the other two could construct the fins/tail design. Your team needs to ensure that the canister lid is kept clear so that it can still come away from the canister during the launch    You will need to do two trials of each (or more if there are launching issues). You should note whether the rocket launched straight up and if their rocket was rolling or was stable. Something being even a fraction off can be the difference between your mission of getting to space succeeding or ending in disaster    You can also have a free design option where you may want to try covering the whole canister with cardboard         Materials         Film canister (or empty water bottle and lid) for the rocket    Protective clothing and safety glasses      Cardboard (of various thickness &amp; textures)    Scissors      Glue or tape (strong tape preferred)    Pencil or pen      Rocket \\u2018fuel\\u2019 from the previous task - baking soda, vinegar, measuring spoons, and water    Paper towel        Do you think you should prioritise height or the straightness of your rocket\\u2019s flight?  Explain your answer   What factors did you have to keep the same in each trial? Why?   Suggest one way in which this experiment could be improved   Which nose cone design worked best? Why?   Make an annotated (labelled) diagram showing your best nose cone and fin designs on your rocket. Make sure that you label each part and explain the feature/features of these parts, what these were made from, and how you attached them to your rocket   Which fin design worked best? Why? \",\n",
      "  \"question\": \"Which fin design worked best? Why?\",\n",
      "  \"rubric\": {\n",
      "    \"criteria\": \"Decide which variables should be changed, measured and controlled in fair tests and accurately observe, measure and record data\",\n",
      "    \"total_score\": 2,\n",
      "    \"items\": [\n",
      "      \"I can explain my response\",\n",
      "      \"I can identify which fin design worked best\",\n",
      "      \"No evidence\"\n",
      "    ]\n",
      "  },\n",
      "  \"answer\": \"Fin designs that have three or four triangular fins that are at the base end of the rocket work best. They stabilise the rocket and make it more aerodynamic or streamlined\"\n",
      "}\n",
      "\n",
      "Preprocessing Statistics:\n",
      "\n",
      "task_content_lengths:\n",
      "  Mean: 4034.21\n",
      "  Max: 11000\n",
      "\n",
      "question_lengths:\n",
      "  Mean: 131.43\n",
      "  Max: 1245\n",
      "\n",
      "answer_lengths:\n",
      "  Mean: 271.56\n",
      "  Max: 1182\n"
     ]
    }
   ],
   "source": [
    "# Verify preprocessing results\n",
    "example_item = processed_data[0]\n",
    "print(\"Example of preprocessed item:\")\n",
    "print(json.dumps(example_item, indent=2))\n",
    "\n",
    "# Calculate preprocessing statistics\n",
    "preprocessed_stats = {\n",
    "    'task_content_lengths': [len(item['task_content']) for item in processed_data],\n",
    "    'question_lengths': [len(item['question']) for item in processed_data],\n",
    "    'answer_lengths': [len(item['answer']) for item in processed_data]\n",
    "    }\n",
    "\n",
    "print(\"\\nPreprocessing Statistics:\")\n",
    "for key, values in preprocessed_stats.items():\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Mean: {mean(values):.2f}\")\n",
    "    print(f\"  Max: {max(values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Format Data for OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of formatted training data:\n",
      "{\n",
      "  \"context\": {\n",
      "    \"task_content\": \"Designing your rocket    Building phase     The shape, weight, and size of a rocket, and it\\u2019s design of nose cone and fins, all affect how aerodynamic or efficient it will be.     Being efficient allows a rocket to use less fuel while travelling long distances or overcoming gravity to take off and escape our atmosphere.     Rockets need to go straight up when launching and not veer to one side or roll when travelling through space. An effective  nose cone  and  fins  will help to stabilise your rocket.     So, to create a rocket that can be launched into space, you must design:     A rocket body    Fins &amp; a nose cone    A final rocket design with all elements     In your teams, you must:     Design your nose cones and fins/tail. Think about which materials to use and how to attach them before constructing their designs    Two team members could construct the nose cone and the other two could construct the fins/tail design. Your team needs to ensure that the canister lid is kept clear so that it can still come away from the canister during the launch    You will need to do two trials of each (or more if there are launching issues). You should note whether the rocket launched straight up and if their rocket was rolling or was stable. Something being even a fraction off can be the difference between your mission of getting to space succeeding or ending in disaster    You can also have a free design option where you may want to try covering the whole canister with cardboard         Materials         Film canister (or empty water bottle and lid) for the rocket    Protective clothing and safety glasses      Cardboard (of various thickness &amp; textures)    Scissors      Glue or tape (strong tape preferred)    Pencil or pen      Rocket \\u2018fuel\\u2019 from the previous task - baking soda, vinegar, measuring spoons, and water    Paper towel        Do you think you should prioritise height or the straightness of your rocket\\u2019s flight?  Explain your answer   What factors did you have to keep the same in each trial? Why?   Suggest one way in which this experiment could be improved   Which nose cone design worked best? Why?   Make an annotated (labelled) diagram showing your best nose cone and fin designs on your rocket. Make sure that you label each part and explain the feature/features of these parts, what these were made from, and how you attached them to your rocket   Which fin design worked best? Why? \",\n",
      "    \"rubric\": {\n",
      "      \"criteria\": \"Decide which variables should be changed, measured and controlled in fair tests and accurately observe, measure and record data\",\n",
      "      \"total_score\": 2,\n",
      "      \"items\": [\n",
      "        \"I can explain my response\",\n",
      "        \"I can identify which fin design worked best\",\n",
      "        \"No evidence\"\n",
      "      ]\n",
      "    },\n",
      "    \"question\": \"Which fin design worked best? Why?\"\n",
      "  },\n",
      "  \"exemplar_answer\": \"Fin designs that have three or four triangular fins that are at the base end of the rocket work best. They stabilise the rocket and make it more aerodynamic or streamlined\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Format preprocessed data for OpenAI API training\n",
    "def prepare_training_format(processed_data):\n",
    "    \n",
    "    formatted_data = []\n",
    "    \n",
    "    for item in processed_data:\n",
    "        formatted_item = {\n",
    "            'context': {\n",
    "                'task_content': item['task_content'],\n",
    "                'rubric': item['rubric'],\n",
    "                'question': item['question']\n",
    "            },\n",
    "            'exemplar_answer': item['answer']\n",
    "            }\n",
    "        formatted_data.append(formatted_item)\n",
    "    \n",
    "    return formatted_data\n",
    "        \n",
    "# Format the preprocessed data\n",
    "formatted_data = prepare_training_format(processed_data)\n",
    "print(\"Example of formatted training data:\")\n",
    "print(json.dumps(formatted_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Split Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 93\n",
      "Validation set size: 24\n",
      "Saved processed training and validation data\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(formatted_data, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Validation set size: {len(val_data)}\")\n",
    "\n",
    "# Save processed and split data\n",
    "with open('data/processed_train_data.json', 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "\n",
    "with open('data/processed_val_data.json', 'w') as f:\n",
    "   json.dump(val_data, f, indent=2)\n",
    "\n",
    "print(\"Saved processed training and validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: OpenAI API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Imports for Section 2\n",
    "\n",
    "import os\n",
    "import tiktoken\n",
    "import time\n",
    "import logging\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Set up API key\n",
    "OPENAI_API_KEY = \"sk-svcacct-uPxqiJzaSiREXSBYOcwrhvmpYLe3uGPMjs6eQ_XELvLftEZ3Ti59ubhaZgPK3Uc0fTU6vevKT3BlbkFJMHPKGyXsJKAVw2CavL0utPanw92cweyJzkuIe4e5v5dtqF803SAACEsEsHfZQnyNWbG-cgUA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Set Up OpeaAI Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Base OpenAI Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOpenAIHandler:\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.total_tokens_used = 0\n",
    "        self.token_limit = 5_000_000\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Initialize token counter file\n",
    "        os.makedirs('outputs', exist_ok=True)\n",
    "        self._init_token_counter()\n",
    "    \n",
    "    def _init_token_counter(self):\n",
    "        \"\"\"Initialize or load token counter from file\"\"\"\n",
    "        try:\n",
    "            with open('outputs/token_usage.json', 'r') as f:\n",
    "                usage_data = json.load(f)\n",
    "                self.total_tokens_used = usage_data.get('total_tokens', 0)\n",
    "        except FileNotFoundError:\n",
    "            self._save_token_usage()\n",
    "    \n",
    "    def _save_token_usage(self):\n",
    "        \"\"\"Save token usage to file\"\"\"\n",
    "        with open('outputs/token_usage.json', 'w') as f:\n",
    "            json.dump({'total_tokens': self.total_tokens_used}, f)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in a text string\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def get_token_usage_stats(self) -> Dict:\n",
    "        \"\"\"Get current token usage statistics\"\"\"\n",
    "        return {\n",
    "            'total_tokens_used': self.total_tokens_used,\n",
    "            'remaining_tokens': self.token_limit - self.total_tokens_used,\n",
    "            'percentage_used': (self.total_tokens_used / self.token_limit) * 100\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 OpenAI Prompt Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptHandler(BaseOpenAIHandler):\n",
    "    def __init__(self, api_key: str, model: str = \"gpt-4o-mini\"):\n",
    "        super().__init__(api_key, model)\n",
    "        self.prompt_template = None\n",
    "        \n",
    "    def format_prompt(self, context: Dict) -> str:\n",
    "        \"\"\"Basic prompt format\"\"\"\n",
    "        prompt = f\"\"\"Given a teaching context, generate a high-quality exemplar answer.\n",
    "\n",
    "Task Content:\n",
    "{context['task_content']}\n",
    "\n",
    "Question:\n",
    "{context['question']}\n",
    "\n",
    "Rubric Criteria:\n",
    "- Total Score: {context['rubric']['total_score']}\n",
    "- Assessment Criteria: {context['rubric']['criteria']}\n",
    "- Scoring Items:\n",
    "{chr(10).join([f\"  {i+1}. {item}\" for i, item in enumerate(context['rubric']['items'])])}\n",
    "\n",
    "Generate an exemplar answer that:\n",
    "1. Directly addresses the question\n",
    "2. Meets the highest scoring criteria in the rubric\n",
    "3. Demonstrates clear understanding and comprehensive coverage\n",
    "4. Uses appropriate academic language\n",
    "5. Is concise yet complete\n",
    "\n",
    "Exemplar Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _create_basic_template(self, context: Dict) -> str:\n",
    "        \"\"\"Basic Prompt Template\"\"\"\n",
    "        return self.format_prompt(context)\n",
    "    \n",
    "    def _create_few_shot_template(self, context: Dict) -> str:\n",
    "        \"\"\"Template with few-shot examples\"\"\"\n",
    "        examples = \"\\n\\n\".join([\n",
    "            f\"Example {i+1}:\\n\"\n",
    "            f\"Question: {ex['question']}\\n\"\n",
    "            f\"Rubric: {ex['rubric']}\\n\"\n",
    "            f\"Answer: {ex['answer']}\\n\"\n",
    "            for i, ex in enumerate(self.best_examples)\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Given a teaching context, generate a high-quality exemplar answer.\n",
    "\n",
    "Previous successful examples:\n",
    "{examples}\n",
    "\n",
    "Now, generate an answer for:\n",
    "Task Content:\n",
    "{context['task_content']}\n",
    "\n",
    "Question:\n",
    "{context['question']}\n",
    "\n",
    "Rubric Criteria:\n",
    "- Total Score: {context['rubric']['total_score']}\n",
    "- Assessment Criteria: {context['rubric']['criteria']}\n",
    "- Scoring Items:\n",
    "{chr(10).join([f\"  {i+1}. {item}\" for i, item in enumerate(context['rubric']['items'])])}\n",
    "\n",
    "Generate an exemplar answer that:\n",
    "1. Directly addresses the question\n",
    "2. Meets the highest scoring criteria in the rubric\n",
    "3. Demonstrates clear understanding and comprehensive coverage\n",
    "4. Uses appropriate academic language\n",
    "5. Is concise yet complete\n",
    "\n",
    "Exemplar Answer:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _create_detailed_template(self, context: Dict) -> str:\n",
    "        \"\"\"Prompt template with detailed instructions\"\"\"\n",
    "        rubric_items = context['rubric']['items']\n",
    "        highest_score_criteria = rubric_items[0]\n",
    "        \n",
    "        prompt = f\"\"\"As an expert education content creator, generate a high-quality exemplar answer.\n",
    "\n",
    "Task Context:\n",
    "{context['task_content']}\n",
    "\n",
    "Question to Answer:\n",
    "{context['question']}\n",
    "\n",
    "To achieve the highest score ({context['rubric']['total_score']} points), your answer must:\n",
    "- Meet this specific criteria: {highest_score_criteria}\n",
    "- Demonstrate deep understanding of: {context['rubric']['criteria']}\n",
    "- Include clear evidence and explanation\n",
    "- Use precise academic language\n",
    "- Be well-structured and coherent\n",
    "\n",
    "Scoring Guide:\n",
    "{chr(10).join([f\"Level {i+1}: {item}\" for i, item in enumerate(rubric_items)])}\n",
    "\n",
    "Additional Requirements:\n",
    "1. Start with a clear main point\n",
    "2. Support with specific evidence\n",
    "3. Explain relationships between concepts\n",
    "4. Use subject-specific vocabulary\n",
    "5. Conclude with a summary if appropriate\n",
    "\n",
    "Your Exemplar Answer:\"\"\"\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 OpenAI Generation Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationHandler(PromptHandler):\n",
    "          \n",
    "    def generate_answer(self, context: Dict, \n",
    "                       max_retries: int = 3, \n",
    "                       temperature: float = 0.7) -> Optional[str]:\n",
    "        \n",
    "        \"\"\"Generate answer with optimized prompt\"\"\"\n",
    "        if self.prompt_template:\n",
    "            prompt = self.prompt_template(context)\n",
    "        else:\n",
    "            prompt = self.format_prompt(context)\n",
    "            \n",
    "        prompt_tokens = self.count_tokens(prompt)\n",
    "        \n",
    "        if self.total_tokens_used + prompt_tokens > self.token_limit:\n",
    "            self.logger.error(\"Token limit reached!\")\n",
    "            return None\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are an expert education content creator specializing in generating exemplar answers for student assessment.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "                \n",
    "                self.total_tokens_used += response.usage.total_tokens\n",
    "                self._save_token_usage()\n",
    "                \n",
    "                return response.choices[0].message.content\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    raise\n",
    "                time.sleep(2 ** attempt)\n",
    "             \n",
    "    def estimate_prompt_tokens(self, context: Dict) -> int:\n",
    "        \"\"\"Estimate tokens for a prompt before sending\"\"\"\n",
    "        formatted_prompt = self.format_prompt(context)\n",
    "        return self.count_tokens(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 OpenAI Training Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingHandler(GenerationHandler):\n",
    "    \n",
    "    def optimize_prompt_template(self, validation_data: List[Dict]):\n",
    "        \"\"\"Optimise prompt templates\"\"\"\n",
    "        templates = [\n",
    "            self._create_basic_template,\n",
    "            self._create_few_shot_template,\n",
    "            self._create_detailed_template\n",
    "        ]\n",
    "        \n",
    "        best_score = 0\n",
    "        best_template = None\n",
    "        \n",
    "        for template_func in templates:\n",
    "            score = self._evaluate_template(template_func, validation_data)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_template = template_func\n",
    "        \n",
    "        self.prompt_template = best_template\n",
    "        self.logger.info(f\"Selected best template with score: {best_score}\")\n",
    "    \n",
    "    def _evaluate_template(self, template_func, validation_data: List[Dict]) -> float:\n",
    "        \"\"\"Evaluating the effectiveness of prompt templates\"\"\"\n",
    "        scores = []\n",
    "        for item in validation_data[:5]:\n",
    "            try:\n",
    "                prompt = template_func(item['context'])\n",
    "                generated_answer = self.generate_answer(item['context'], temperature=0.3)\n",
    "                if generated_answer:\n",
    "                    quality_score = self._evaluate_example_quality({\n",
    "                        'answer': generated_answer,\n",
    "                        'rubric': json.dumps(item['context']['rubric']),\n",
    "                        'question': item['context']['question']\n",
    "                    })\n",
    "                    scores.append(quality_score)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Template evaluation error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return np.mean(scores) if scores else 0.0\n",
    "    \n",
    "    def _evaluate_example_quality(self, example: Dict) -> float:\n",
    "        \"\"\"Example quality assessment\"\"\"\n",
    "        score = 0.0\n",
    "        answer = example['answer']\n",
    "        rubric = example['rubric']\n",
    "        rubric_data = json.loads(rubric)\n",
    "        \n",
    "        # 1. Content relevance (0.6 points)\n",
    "        rubric_items = rubric_data['items']\n",
    "        criteria = rubric_data['criteria'].lower()\n",
    "        answer_lower = answer.lower()\n",
    "        \n",
    "        # Check for keyword matching\n",
    "        keyword_matches = sum(1 for item in rubric_items if item.lower() in answer_lower)\n",
    "        content_score = min(0.4, (keyword_matches / len(rubric_items)) * 0.5)\n",
    "        score += content_score\n",
    "        \n",
    "        # Completeness of answer (0.2 marks)\n",
    "        # Adjustment of desired length based on total score\n",
    "        expected_length = int(rubric_data['total_score']) * 50  # 每分50字符\n",
    "        actual_length = len(answer)\n",
    "        if actual_length >= expected_length:\n",
    "            score += 0.3\n",
    "        else:\n",
    "            score += 0.3 * (actual_length / expected_length)\n",
    "        \n",
    "        # 3. Language quality (0.2 points)\n",
    "        sentences = [s.strip() for s in answer.split('.') if s.strip()]\n",
    "        \n",
    "        # Sentence count scoring\n",
    "        if len(sentences) >= 2:\n",
    "            score += 0.1\n",
    "        \n",
    "        # Lexical diversity\n",
    "        words = answer.split()\n",
    "        unique_words = set(words)\n",
    "        vocabulary_ratio = len(unique_words) / len(words) if words else 0\n",
    "        if vocabulary_ratio >= 0.6:  # Lexical richness thresholds\n",
    "            score += 0.1\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def select_best_examples(self, training_data: List[Dict], n_examples: int = 3):\n",
    "        \"\"\"Select the best few-shot example\"\"\"\n",
    "        selected_examples = []\n",
    "        for item in training_data:\n",
    "            \n",
    "            example = {\n",
    "                'answer': item['exemplar_answer'],\n",
    "                'rubric': json.dumps(item['context']['rubric']),\n",
    "                'question': item['context']['question']\n",
    "            }\n",
    "            # Assessing quality\n",
    "            quality_score = self._evaluate_example_quality(example)\n",
    "            example['quality_score'] = quality_score\n",
    "            selected_examples.append(example)\n",
    "        \n",
    "        # Sort and select the best examples\n",
    "        selected_examples.sort(key=lambda x: x['quality_score'], reverse=True)\n",
    "        self.best_examples = selected_examples[:n_examples]\n",
    "        \n",
    "        # Save the best examples\n",
    "        with open('outputs/best_examples.json', 'w') as f:\n",
    "            json.dump(self.best_examples, f, indent=2)\n",
    "            \n",
    "    def train(self, training_data: List[Dict], validation_data: List[Dict]):\n",
    "            \"\"\"Training process\"\"\"\n",
    "            self.logger.info(\"Starting training process...\")\n",
    "            \n",
    "            #  Select the best example\n",
    "            self.logger.info(\"Selecting best examples...\")\n",
    "            self.select_best_examples(training_data)\n",
    "            \n",
    "            # Optimise the prompt template\n",
    "            self.logger.info(\"Optimizing prompt template...\")\n",
    "            self.optimize_prompt_template(validation_data)\n",
    "            \n",
    "            # Save the training results\n",
    "            self.logger.info(\"Saving training results...\")\n",
    "            self._save_training_results()\n",
    "            \n",
    "            self.logger.info(\"Training completed!\")\n",
    "    \n",
    "    def _save_training_results(self):\n",
    "        \"\"\"Save training results to file\"\"\"\n",
    "        results = {\n",
    "            'best_examples': self.best_examples,\n",
    "            'selected_template': self.prompt_template.__name__,\n",
    "            'token_usage': self.get_token_usage_stats()\n",
    "        }\n",
    "        \n",
    "        with open('outputs/training_results.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Final OpenAI Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIHandler(TrainingHandler):\n",
    "    \"\"\"\n",
    "    Final OpenAIHandler class that inherits all functionality\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train the OpenAI API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training process...\n",
      "INFO:__main__:Selecting best examples...\n",
      "INFO:__main__:Optimizing prompt template...\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Selected best template with score: 0.43999999999999995\n",
      "INFO:__main__:Saving training results...\n",
      "INFO:__main__:Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the handler\n",
    "handler = OpenAIHandler(OPENAI_API_KEY)\n",
    "\n",
    "handler.train(train_data, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Test the OpenAI API Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Test on the First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tokens for first example: 662\n"
     ]
    }
   ],
   "source": [
    "# Test the number of tokens with the first example from our processed data\n",
    "test_context = formatted_data[0]['context']\n",
    "\n",
    "# Estimate tokens\n",
    "estimated_tokens = handler.estimate_prompt_tokens(test_context)\n",
    "print(f\"Estimated tokens for first example: {estimated_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: **Which fin design worked best? Why?**\n",
      "\n",
      "In our rocket design experiment, the fin design that worked best was the triangular fin configuration. This design outperformed the others in terms of stability and straightness of flight during the launch trials. \n",
      "\n",
      "The triangular fins were strategically placed at an angle, which allowed for optimal airflow around the rocket. This aerodynamic shape minimized drag, enabling the rocket to ascend more efficiently. Throughout the trials, we observed that rockets equipped with triangular fins maintained a much straighter trajectory compared to those with rectangular or circular fins. Specifically, the triangular fins reduced the tendency of the rocket to veer to one side or roll, which are critical factors for achieving successful launches. \n",
      "\n",
      "In terms of variables, we controlled the size and weight of the fins, ensuring that all designs were made from the same material—lightweight cardboard. This allowed us to isolate the fin shape as the primary variable affecting performance. We measured the angle of ascent and the distance traveled for each trial, recording data meticulously to ensure accuracy. The triangular fin consistently yielded an average ascent angle of approximately 85 degrees, compared to the rectangular fins, which averaged around 75 degrees, indicating that the triangular design provided better lift and stability.\n",
      "\n",
      "Additionally, the attachment method we used for the triangular fins involved strong tape, which secured them firmly to the rocket body, minimizing any potential wobbling that could occur with less stable designs. This method of attachment further contributed to the overall effectiveness of the triangular fin design.\n",
      "\n",
      "In conclusion, the triangular fin design was the most successful due to its aerodynamic shape, which facilitated a stable and straight flight path. This choice not only enhanced the rocket's performance but also underscored the importance of fin design in achieving an efficient launch. Thus, in future iterations of our project, we will continue to utilize and refine this fin design to further improve our rocket's capabilities.\n"
     ]
    }
   ],
   "source": [
    "# Run the test on the first example\n",
    "test_context = formatted_data[0]['context']\n",
    "test_answer = handler.generate_answer(test_context)\n",
    "print(\"Generated answer:\", test_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage statistics:\n",
      "{\n",
      "  \"total_tokens_used\": 145584,\n",
      "  \"remaining_tokens\": 4854416,\n",
      "  \"percentage_used\": 2.91168\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check token usage\n",
    "usage_stats = handler.get_token_usage_stats()\n",
    "\n",
    "print(\"Token usage statistics:\")\n",
    "print(json.dumps(usage_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Batch Processing Function\n",
    "\n",
    "Create a function to process multiple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(handler, data_batch, batch_size=5):\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i + batch_size]\n",
    "        \n",
    "        # Process each item in the batch\n",
    "        for item in batch:\n",
    "            try:\n",
    "                result = handler.generate_answer(item['context'])\n",
    "                results.append(result)\n",
    "                print(f\"Processed item {len(results)}\")\n",
    "                \n",
    "                # Save results to file\n",
    "                with open(f'outputs/generated_answers_batch_{i//batch_size + 1}.json', 'w') as f:\n",
    "                    json.dump(results, f, indent=2)\n",
    "                \n",
    "                # Check token usage\n",
    "                usage_stats = handler.get_token_usage_stats()\n",
    "                print(f\"Remaining tokens: {usage_stats['remaining_tokens']}\")\n",
    "                \n",
    "                # Sleep for a second to avoid rate limits\n",
    "                time.sleep(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {e}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed item 1\n",
      "Remaining tokens: 4853376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed item 2\n",
      "Remaining tokens: 4851765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed item 3\n",
      "Remaining tokens: 4850208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed item 4\n",
      "Remaining tokens: 4848826\n",
      "\n",
      "Test batch results:\n",
      "\n",
      "Example 1:\n",
      "**Exemplar Answer: Which fin design worked best? Why?**\n",
      "\n",
      "The fin design that proved to be the most effective in stabilizing our rocket during flight was the triangular fin configuration. This design not only enhanced the rocket's stability but also ensured that it maintained a straight trajectory during launch, which is crucial for achieving optimal height and aerodynamics.\n",
      "\n",
      "**Supporting Evidence:**\n",
      "During our trials, the triangular fins demonstrated superior performance compared to other designs, such as rectangular and square fins. Specifically, when we conducted our first trial with the triangular fins, the rocket ascended vertically with minimal lateral movement, reaching an altitude of approximately 15 meters. In contrast, the rectangular fins led to significant rolling and veering off course, resulting in a maximum altitude of only 10 meters.\n",
      "\n",
      "**Explanation of Relationships:**\n",
      "The effectiveness of the triangular fin design can be attributed to its aerodynamic shape, which reduces drag during flight. The pointed edges of the triangular fins effectively cut through the air, minimizing turbulence and allowing for a smoother ascent. Additionally, the triangular configuration creates a stabilizing force that counters any unwanted rotational motion, keeping the rocket aligned with its intended path. This relationship between fin shape and flight stability is critical, as any deviation from a straight trajectory can lead to decreased performance and efficiency.\n",
      "\n",
      "**Conclusion:**\n",
      "In summary, the triangular fin design was the most successful in our rocket trials due to its aerodynamic advantages and enhanced stability, which allowed the rocket to maintain a straight flight path and achieve a higher altitude. This design emphasizes the importance of fin shape in rocket aerodynamics, demonstrating that thoughtful engineering choices can significantly influence mission success.\n",
      "\n",
      "Example 2:\n",
      "In our solar system, Mercury possesses the fastest orbit, completing a full revolution around the Sun in just 88 Earth days, while Neptune holds the title for the slowest orbit, taking approximately 60,190 Earth days to complete one revolution. The disparity in orbital speeds between these two planets can primarily be attributed to their respective distances from the Sun, as well as the gravitational forces at play.\n",
      "\n",
      "Mercury, being the closest planet to the Sun, experiences a stronger gravitational pull, which influences its rapid orbital speed. According to Kepler's laws of planetary motion, planets closer to the Sun travel at higher velocities due to the increased gravitational attraction. This relationship can be seen in the elliptical shape of Mercury's orbit, which, although slightly elongated, maintains a general proximity to the Sun that facilitates its swift revolution.\n",
      "\n",
      "In contrast, Neptune, located farthest from the Sun, orbits at a significantly slower pace. The weaker gravitational force at this greater distance results in a longer orbital period. Neptune’s orbit is also elliptical, but the effects of solar gravity diminish as the distance increases, leading to a slower velocity. Consequently, the time it takes for Neptune to complete one full revolution is vastly extended compared to that of Mercury.\n",
      "\n",
      "Additionally, the mass of the planets and their respective distances from the Sun play crucial roles in determining their orbital speeds. Mercury's minimal axial tilt of about 2 degrees further contributes to its stable and rapid orbit, while Neptune’s axial tilt of 28.5 degrees, though not the most extreme, does not impact its orbital speed as significantly as its distance from the Sun.\n",
      "\n",
      "In summary, the fastest orbit belongs to Mercury, completing its revolution in 88 Earth days, while Neptune’s slow orbit spans around 60,190 Earth days. This variation in orbital speed is fundamentally influenced by gravitational forces linked to their distances from the Sun, illustrating the interconnectedness of celestial mechanics within our solar system.\n",
      "\n",
      "Example 3:\n",
      "**Exemplar Answer: Properties of Solids, Liquids, and Gases Based on Shape, Volume, and Compressibility**\n",
      "\n",
      "In understanding the properties of solids, liquids, and gases, it is essential to consider the particle model, which describes the arrangement and behavior of particles in different states of matter. Each state has distinct characteristics that can be categorized into three properties: shape, volume, and compressibility.\n",
      "\n",
      "**1. Solids**\n",
      "- **Shape:** Solids have a definite shape due to the closely packed arrangement of their particles. In solids, particles are tightly held together by strong intermolecular forces and vibrate in fixed positions, which prevents them from moving freely. This rigidity accounts for the solid’s fixed shape.\n",
      "- **Volume:** Solids possess a fixed volume. The particles are arranged in a structured lattice, meaning that they cannot be compressed or altered significantly in volume under normal conditions.\n",
      "- **Compressibility:** Solids are generally incompressible. The strong attraction between the particles means that when pressure is applied, the particles do not move closer together, maintaining their volume and shape.\n",
      "\n",
      "**2. Liquids**\n",
      "- **Shape:** Unlike solids, liquids do not have a definite shape; they take the shape of their container. The particles in a liquid are still closely packed but can slide past one another, allowing the liquid to flow and conform to the shape of its surroundings.\n",
      "- **Volume:** Liquids maintain a fixed volume, similar to solids. While liquids can flow, the volume remains constant because the particles are not easily compressible and remain in close proximity.\n",
      "- **Compressibility:** Liquids are also considered incompressible under normal conditions. The particles are close together, and applying pressure does not significantly decrease the space between them.\n",
      "\n",
      "**3. Gases**\n",
      "- **Shape:** Gases have neither a definite shape nor a fixed volume. They will expand to fill any container they occupy, as the particles are far apart and move freely.\n",
      "- **Volume:** Gases do not have a fixed volume; instead, they will expand to occupy the entire volume of their container. This is because the particles are in constant random motion and are much less densely packed than in solids and liquids.\n",
      "- **Compressibility:** Gases are highly compressible. The large spaces between particles allow them to be pushed closer together when pressure is applied, significantly reducing their volume.\n",
      "\n",
      "**Conclusion**\n",
      "In summary, the particle model provides a clear framework to understand the distinct properties of solids, liquids, and gases. Solids have a definite shape and volume with minimal compressibility, liquids take the shape of their container while maintaining a fixed volume and are also incompressible, and gases have no fixed shape or volume and are highly compressible. These properties arise from the arrangement and movement of particles, which are directly influenced by the intermolecular forces at play in each state of matter.\n",
      "\n",
      "Example 4:\n",
      "To determine the ratio of the load force to the effort force in this experiment, we first need to identify the masses used for both the load and effort. In this case, we utilized a 200g mass as the load and a 100g mass as the effort.\n",
      "\n",
      "To express the ratio of the load force to the effort force, we can use the following formula:\n",
      "\n",
      "\\[\n",
      "\\text{Ratio of Load Force to Effort Force} = \\frac{\\text{Mass of Load}}{\\text{Mass of Effort}} = \\frac{200g}{100g} = 2:1\n",
      "\\]\n",
      "\n",
      "This calculation indicates that the load force is twice as large as the effort force applied. \n",
      "\n",
      "We know this ratio holds true because, according to the principles of levers and moments, the torque (or moment) about the pivot must be balanced for equilibrium to occur. The moment is calculated using the formula:\n",
      "\n",
      "\\[\n",
      "\\text{Torque} = \\text{Force} \\times \\text{Distance from Pivot}\n",
      "\\]\n",
      "\n",
      "In our experiment, when the 200g load was placed at the 70cm mark from the pivot, the torque generated by the load was:\n",
      "\n",
      "\\[\n",
      "\\text{Torque}_{\\text{load}} = 200g \\times 70cm = 14000 \\, \\text{g cm}\n",
      "\\]\n",
      "\n",
      "On the other side, we adjusted the 100g effort mass to achieve balance by varying its distance from the pivot. When the 100g mass is positioned at the appropriate distance, its torque must equal that of the load to maintain balance:\n",
      "\n",
      "\\[\n",
      "\\text{Torque}_{\\text{effort}} = 100g \\times \\text{Distance}_{\\text{effort}}\n",
      "\\]\n",
      "\n",
      "For example, if the effort mass was positioned at the 1m mark (100cm from the pivot), its torque would be:\n",
      "\n",
      "\\[\n",
      "\\text{Torque}_{\\text{effort}} = 100g \\times 100cm = 10000 \\, \\text{g cm}\n",
      "\\]\n",
      "\n",
      "To maintain equilibrium, the torques must balance each other. Thus, the relationship between the load distance and the effort distance reflects the inverse relationship between the magnitudes of the forces applied. \n",
      "\n",
      "In conclusion, our findings indicate a clear relationship between the load force and effort force, substantiated by our experimental setup and calculations. The ratio of the load force to effort force is 2:1, which is consistent with the underlying principles of levers, demonstrating how a greater load can be balanced by a smaller effort when positioned appropriately from the pivot.\n"
     ]
    }
   ],
   "source": [
    "# Process a small test batch\n",
    "test_batch = formatted_data[:4]\n",
    "test_results = process_batch(handler, test_batch, batch_size=1)\n",
    "\n",
    "print(\"\\nTest batch results:\")\n",
    "for i, result in enumerate(test_results):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token usage statistics:\n",
      "{\n",
      "  \"total_tokens_used\": 151174,\n",
      "  \"remaining_tokens\": 4848826,\n",
      "  \"percentage_used\": 3.0234799999999997\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check token usage\n",
    "usage_stats = handler.get_token_usage_stats()\n",
    "\n",
    "print(\"Token usage statistics:\")\n",
    "print(json.dumps(usage_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Exemplar Answer Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency Imports for Section 3\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import spacy\n",
    "import textstat\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CuraProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
